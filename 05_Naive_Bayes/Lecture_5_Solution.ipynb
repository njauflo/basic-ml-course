{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6sFU3yy9rKe"
      },
      "source": [
        "In this assignment, we will implement Bernoulli Naive Bayes and Multinomial Naive Bayes, and apply them for text classification. \\\\\n",
        "We will experiment on the 20 newsgroups text dataset. It comprises around 18000 newsgroups posts on 20 topics split in two subsets: one for training (or development) and the other one for testing (or for performance evaluation). \\\\\n",
        "First, we load the dataset from sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VUJshdxZ8N7G"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXgEMdjf-drl"
      },
      "source": [
        "You can access to text by `data` property. For labels, their name and corresponding numeric values are stored in `target_names` and `target`. \\\\\n",
        "Let's take a look at our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pri0JqOR-mP_",
        "outputId": "1a9dc8af-30f6-4376-8913-55b455db7468"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11314"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(newsgroups_train.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWzOQVje-0OI",
        "outputId": "582effa7-7d06-469c-8052-398033d8b013"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([7, 4, 4, ..., 3, 1, 8]),\n",
              " ['alt.atheism',\n",
              "  'comp.graphics',\n",
              "  'comp.os.ms-windows.misc',\n",
              "  'comp.sys.ibm.pc.hardware',\n",
              "  'comp.sys.mac.hardware',\n",
              "  'comp.windows.x',\n",
              "  'misc.forsale',\n",
              "  'rec.autos',\n",
              "  'rec.motorcycles',\n",
              "  'rec.sport.baseball',\n",
              "  'rec.sport.hockey',\n",
              "  'sci.crypt',\n",
              "  'sci.electronics',\n",
              "  'sci.med',\n",
              "  'sci.space',\n",
              "  'soc.religion.christian',\n",
              "  'talk.politics.guns',\n",
              "  'talk.politics.mideast',\n",
              "  'talk.politics.misc',\n",
              "  'talk.religion.misc'])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "newsgroups_train.target, newsgroups_train.target_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "2RDyDW0N-XGU",
        "outputId": "9055103c-5678-4377-b4e7-2052792549aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "newsgroups_train.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3AQxoDZ_hBe"
      },
      "source": [
        "When applying machine learning to solve problems, designing algorithm is not the only way to optimize. We can also intervene on data, i.e. data preprocessing, feature selection, etc. In some case, this approach is even better than model optimizing. \\\\\n",
        "For this dataset, you can notice that the text have lots of redundant information, for example punctuation, title, etc. We can remove those from our data to get better performance. Here I define a function to remove all punctuation from text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "d1wQ6mFp-YyE"
      },
      "outputs": [],
      "source": [
        "def remove_tokens(token_list, text):\n",
        "    for token in token_list:\n",
        "        text = text.replace(token, '')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yw6KGy8pBPUr"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "preprocessed_text = [remove_tokens(punctuation, text) for text in newsgroups_train.data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK2T5rTQBZEY"
      },
      "source": [
        "**Assignment 1** : First we have to transform text into numeric feature. You have to build a matrix that counts word occurences in each documents **(0.5pt)**. For fast computing, we only select `30000` words with highest frequency. \\\\\n",
        "*Hint* You should use `sklearn.feature_extraction.text.CountVectorizer` and `max_features` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "426ZmPR-ytRL",
        "outputId": "b652b381-3e0c-495a-d95e-3984bf91d3fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "num_word = 30000\n",
        "vectorizer = CountVectorizer(max_features = num_word)\n",
        "train_data = vectorizer.fit_transform(preprocessed_text).toarray()\n",
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnHe2WcpDRJ_"
      },
      "source": [
        "Recall that for Naive Bayes, we find label that satisfy \n",
        "$$c=\\arg\\max_{c'} p(c')\\prod p(x_i|c')$$\n",
        "**Assigment 2** : We will derive prior probabilities $p(c')$ from data by computing frequency of class. You have to compute the number of documents in each class in `class_freq` variable, and divide to the total number of documents to get prior probability in `prior_prob` variable **(1pt)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tyufOD_ODQra"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "classes,class_freq = np.unique(newsgroups_train.target,return_counts = True)\n",
        "prior_prob = class_freq / np.sum(class_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib3LX3zVGe0L"
      },
      "source": [
        "**Assigment 3** : In this step, we will implement Bernoulli Naive Bayes. Therefore, the conditional probability is probability of that a document with label $c$ has the word $x_i$. \\\\\n",
        "To do that, we need the number of documents which has word $x_i$ and label $c$ for every pair $(x_i,c)$. Your task is computing these values and store them in `word_label_freq` variable. It should be a numpy array for fast computing in the next step. **(0.5.pt)** \\\\\n",
        "*Hint:* Our `'train_data` features are the number of occurences of words in documents. You can convert them to binary feature that whether a word appears in a document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "x5c39g3zFKX6",
        "outputId": "c09f9b3b-dd37-4b84-cd2d-78238456663b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.00526316, 0.01052632, 0.01578947, 0.02105263,\n",
              "       0.02631579, 0.03157895, 0.03684211, 0.04210526, 0.04736842,\n",
              "       0.05263158, 0.05789474, 0.06315789, 0.06842105, 0.07368421,\n",
              "       0.07894737, 0.08421053, 0.08947368, 0.09473684, 0.1       ])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "word_label_freq = classes/ np.sum(classes,axis = 0)\n",
        "word_label_freq "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDRlWLSdGiXu"
      },
      "source": [
        "**Assigment 4** : \n",
        "The conditional probability is computed by dividing the number of documents which has word $x_i$ and label $c$ to the number of documents with label $c$. However, if there is no document which has word $x_i$ and label $c$ in training data, the probability will be zero, which is undesirable. \\\\\n",
        "To handle this problem, we can apply Laplace smoothing, then conditional probability will be computed as following\n",
        "$$p(x_i=1|c) = \\frac{N_{ic} + \\alpha}{N_c +|V|\\alpha}$$\n",
        "Your task here is implementing this formula with default `alpha=0.1` and then fill in all the probability values in `cond_prob` variable. It should be a numpy array for fast computing in the next step. **(1pt)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "015-PXOs2qSl"
      },
      "outputs": [],
      "source": [
        "alpha = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "cH_GMi4nFSzk",
        "outputId": "95cafae1-c7b3-49ad-88d4-75da568c6469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.28205128e-05, 1.72660157e-05, 2.30373915e-05, 2.89769367e-05,\n",
              "       3.53674619e-05, 4.06671775e-05, 4.69818614e-05, 5.23960909e-05,\n",
              "       5.80236783e-05, 6.39558763e-05, 6.95906433e-05, 7.58600412e-05,\n",
              "       8.21076260e-05, 8.77192982e-05, 9.37113220e-05, 9.89403431e-05,\n",
              "       1.11359960e-04, 1.15131579e-04, 1.36910905e-04, 1.62481536e-04])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "cond_prob =  np.array([(word_label_freq[i] + alpha)/(class_freq[i] + num_word * alpha) for i in range(len(class_freq))]) \n",
        "cond_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsBHViWqGj9U"
      },
      "source": [
        "**Assigment 5** : For test data, the conditional probabily follows Bernoulli distribution and is computed by\n",
        "$$p(x_i|c) = p(x_i=1|c)x_i + p(x_i=0|c)(1 - x_i)$$\n",
        "Then we multiply with prior probability and select the class with highest value as predicted label. For numerical stability, you should use log probability to compute. **(2pt)** \\\\\n",
        "$$c = \\arg\\max_{c'}(\\log p(c')+\\sum_{x_i\\in text}\\log p(x_i|c'))$$\n",
        "*Hint* Remember to convert test data feature to binary feature as training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZbLeeFp43Eg7"
      },
      "outputs": [],
      "source": [
        "def find_label(data):\n",
        "    data = np.array(data.toarray()).flatten()\n",
        "    indices = np.where(data > 0)\n",
        "    indices_0 = np.where(data==0)\n",
        "  \n",
        "    mx_val = -1e18\n",
        "    result_class = 0\n",
        "    for i,class_val in enumerate(cond_prob):\n",
        "      prod = np.log(prior_prob[i]) + np.sum(np.log(class_val[indices])) + np.sum(np.log(1 - class_val[indices_0]))\n",
        "      if prod >= mx_val:\n",
        "        result_class = i\n",
        "        mx_val = prod\n",
        "    return result_class "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epm5l9atNAMr"
      },
      "source": [
        "Now we can obtain labels and accuracy score of model on test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "bxBsRI6f4Jym"
      },
      "outputs": [],
      "source": [
        "preprocessed_test_text = [remove_tokens(punctuation, text) for text in newsgroups_test.data]\n",
        "test_data = vectorizer.transform(preprocessed_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNLWpQMc4a9s",
        "outputId": "415ccc4a-543f-4f4b-f914-30100054a1f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "7532it [00:00, 19620.03it/s]\n"
          ]
        }
      ],
      "source": [
        "pred = []\n",
        "from tqdm import tqdm\n",
        "for text in tqdm(test_data):\n",
        "    pred.append(find_label(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "MSEKT9t-7KgT",
        "outputId": "be07dca2-d88e-4c40-c240-9de3e2f1f62d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-f454cf9a8d2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewsgroups_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    332\u001b[0m         raise ValueError(\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         )\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7532, 5]"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "metrics.accuracy_score(pred, newsgroups_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x70hGH3RGlbI"
      },
      "source": [
        "**Assigment 6** : Next, we will implement Multinomial Naive Bayes. For this model, the conditional probability follows multinomial distribution. We have to estimate conditional probabilities from data as the frequency of words in all documents with a given class. \\\\\n",
        "First, you have to count the number of occurences of a word $x_i$ in all document with class $c$ **(0.5pt)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZGCBLk8FaHw"
      },
      "outputs": [],
      "source": [
        "word_label_freq = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GP4fhl_GsOQ"
      },
      "source": [
        "**Assigment 7** : Your task here is to count the total number of words in all documents of class $c$ to compute probability in next step. **(0.5)pt** \\\\\n",
        "*Hint* You can sum the number of occurences of all words in class $c$ that we obtained in last step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2yAJ49sFud0"
      },
      "outputs": [],
      "source": [
        "num_word_in_classes = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S4qCZrtGtwO"
      },
      "source": [
        "**Assigment 8** : Now we can compute conditional probability for every pair $(x_i, c)$. Similar to Bernoulli Naive Bayes, we also add Laplace smoothing to avoid zero probability\n",
        "$$p(x_i|c) = \\frac{N_{ic} + \\alpha}{N_c +|V|\\alpha}$$\n",
        "where $N_{ic}$ is the number of occurences of word $x_i$ in all documents with class $c$, $N_c$ is the total number of words in all documents of class $c$. \\\\\n",
        "For numerical stability, you should compute log of probablity **(1pt)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUMX_iwDFwr3"
      },
      "outputs": [],
      "source": [
        "log_cond_prob = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whEfFBPC-W2H"
      },
      "source": [
        "**Assignment 9** : After getting all necessary probabilities, we can find label of new test data. For this task, you have to implement `find_label` function that compute product of prior and conditional probablities, and select the label with highest value. Finally, you can get prediction for all test data and get accuracy score **(3pt)**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7pylT3i_CiO"
      },
      "outputs": [],
      "source": [
        "def find_label(data):\n",
        "    \"\"\"\n",
        "      Your code here\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHolEq4mb3Jc"
      },
      "outputs": [],
      "source": [
        "pred = []\n",
        "for text in tqdm(test_data):\n",
        "    pred.append(find_label(text))\n",
        "metrics.accuracy_score(pred, newsgroups_test.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU9JwQ85N6bv"
      },
      "source": [
        "**(Optional)** Try to improve performance of Naive Bayes model in this dataset. You can try everything to do this, i.e. change hyperparameters, preprocess data, ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}